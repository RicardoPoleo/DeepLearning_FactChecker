{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhG5ZCpVGn/ii9wF40fjiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicardoPoleo/DeepLearning_FactChecker/blob/main/notebooks/Agents/WebService_Agent_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MOXhtj95YBC"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Manual imports\n",
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "wWviye375qt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def login_huggingface():\n",
        "    from google.colab import userdata\n",
        "    from huggingface_hub import login\n",
        "    hf_token = userdata.get('hg_token')\n",
        "    login(token=hf_token)\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "\n",
        "class OurFineTuner:\n",
        "    def __init__(self, dataset_filepath, dataset_type=\"csv\"):\n",
        "        self.training_stats = None\n",
        "        self.trainer = None\n",
        "        self.max_seq_length = 2048\n",
        "        self.instructions_format = \"\"\n",
        "        self.dataset_filepath = dataset_filepath\n",
        "        self.dataset_type = dataset_type\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.dataset = None\n",
        "        self.train_dataset = None\n",
        "        self.validation_dataset = None\n",
        "\n",
        "    def pick_model(self, model_name):\n",
        "        dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "        load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=model_name,\n",
        "            max_seq_length=self.max_seq_length,\n",
        "            dtype=dtype,\n",
        "            load_in_4bit=load_in_4bit,\n",
        "        )\n",
        "        self.add_qlora()\n",
        "\n",
        "    def add_qlora(self):\n",
        "        self.model = FastLanguageModel.get_peft_model(\n",
        "            self.model,\n",
        "            r=16,\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0,\n",
        "            bias=\"none\",\n",
        "            use_gradient_checkpointing=\"unsloth\",\n",
        "            random_state=3407,\n",
        "            use_rslora=False,\n",
        "            loftq_config=None,\n",
        "        )\n",
        "\n",
        "    def load_dataset(self):\n",
        "        if self.dataset_type == \"csv\":\n",
        "            self.dataset = load_dataset(\"csv\", data_files=self.dataset_filepath, split=\"train\")\n",
        "        elif self.dataset_type == \"HuggingFace\":\n",
        "            self.dataset = load_dataset(self.dataset_filepath)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dataset type: {self.dataset_type}\")\n",
        "\n",
        "    def format_data(self, test_size=0.2):\n",
        "        split_dataset = self.dataset.train_test_split(test_size=test_size)\n",
        "        self.train_dataset = split_dataset['train']\n",
        "        self.validation_dataset = split_dataset['test']\n",
        "        self.train_dataset = self.train_dataset.map(self.formatting_prompts_func, batched=True)\n",
        "        self.validation_dataset = self.validation_dataset.map(self.formatting_prompts_func, batched=True)\n",
        "\n",
        "    def formatting_prompts_func(self, examples):\n",
        "        EOS_TOKEN = self.tokenizer.eos_token\n",
        "        texts = [self.instructions_format.format(ex['instruction'], ex['input'], ex['output']) + EOS_TOKEN for ex in zip(examples['instruction'], examples['input'], examples['output'])]\n",
        "        return {\"text\": texts}\n",
        "\n",
        "    def prepare_trainer(self, max_steps=60):\n",
        "        self.trainer = SFTTrainer(\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            train_dataset=self.train_dataset,\n",
        "            dataset_text_field=\"text\",\n",
        "            max_seq_length=self.max_seq_length,\n",
        "            dataset_num_proc=2,\n",
        "            packing=False,\n",
        "            args=TrainingArguments(\n",
        "                per_device_train_batch_size=2,\n",
        "                gradient_accumulation_steps=4,\n",
        "                warmup_steps=5,\n",
        "                max_steps=max_steps,\n",
        "                learning_rate=2e-4,\n",
        "                fp16=not is_bfloat16_supported(),\n",
        "                bf16=is_bfloat16_supported(),\n",
        "                logging_steps=1,\n",
        "                optim=\"adamw_8bit\",\n",
        "                weight_decay=0.01,\n",
        "                lr_scheduler_type=\"linear\",\n",
        "                seed=3407,\n",
        "                output_dir=\"outputs\",\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def perform_inference(self, instruction, claim, explanation):\n",
        "        input_text = f\"Claim: {claim}. Explanation: {explanation}.\"\n",
        "        inputs = self.tokenizer(\n",
        "            self.instructions_format.format(instruction, input_text, \"\"),\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "        outputs = self.model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "        response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "    def set_instructions_format(self, instructions_format=\"\"):\n",
        "        if instructions_format == \"\":\n",
        "            self.instructions_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "        else:\n",
        "            self.instructions_format = instructions_format\n"
      ],
      "metadata": {
        "id": "s9VoJEJ45wEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create the instance of the model for fast inference\n",
        "dataset_path = \"https://github.com/RicardoPoleo/DeepLearning_FactChecker/raw/main/datasets/3rd-attempt-input-instruction-claim-veredict-output-veredict.csv\"\n",
        "finetuner = OurFineTuner(dataset_filepath=dataset_path, dataset_type=\"csv\")  # Assuming OurFineTuner class is already defined/imported\n",
        "finetuner.pick_model(model_name)\n",
        "finetuner.set_instructions_format()"
      ],
      "metadata": {
        "id": "JB1sXA2n82M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def do_inference(model_name, instruction, claim, explanation):\n",
        "    print(f\"=== Inference with the model: {model_name}\")\n",
        "    response = finetuner.perform_inference(instruction, claim, explanation)\n",
        "    print(response)\n",
        "    return response"
      ],
      "metadata": {
        "id": "ZHYdzWhI5iNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start the Web Service\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "import subprocess\n",
        "import threading\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class RequestModel(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"inference\")\n",
        "def inference(request: RequestModel):\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "    data = json.loads(request.json())\n",
        "    instruction = \"You are a fact-checker AI. Evaluate the following claim with its explanation and, based on the provided information, determine whether or not the claim is true or not, followed by the explanation of why.\"\n",
        "    claim = data[\"claim\"]\n",
        "    explanation = data[\"explanation\"]\n",
        "    response = do_inference(model_name, instruction, claim, explanation)\n",
        "    return {\"response\": response}\n",
        "\n",
        "\n",
        "\n",
        "def start_uvicorn():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Free the port before starting the server\n",
        "!fuser -k 8000/tcp\n",
        "\n",
        "thread = threading.Thread(target=start_uvicorn)\n",
        "thread.start()\n",
        "\n",
        "process = subprocess.Popen([\"lt\", \"--port\", \"8000\"], stdout=subprocess.PIPE)\n",
        "for line in process.stdout:\n",
        "    print(line.decode().strip())"
      ],
      "metadata": {
        "id": "i9530tEU6RKT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}